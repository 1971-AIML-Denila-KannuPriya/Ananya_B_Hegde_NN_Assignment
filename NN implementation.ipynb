{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c880932-35c2-4099-bac6-ec1d1e4197fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. TensorBoard:\n",
    "•\tA visualization toolkit for tracking and visualizing machine learning metrics.\n",
    "•\tExample:\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import datetime\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test), callbacks=[tensorboard_callback])\n",
    "2. Layer Types:\n",
    "•\tDense, Convolutional, Recurrent layers for various tasks.\n",
    "3. Sequential API:\n",
    "•\tBuild models layer-by-layer in a linear stack.\n",
    "•\tExample:\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "4. Functional API:\n",
    "•\tCreate complex architectures with shared layers.\n",
    "•\tExample:\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "inputs = tf.keras.Input(shape=(32,))\n",
    "x = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "     \n",
    "5. Adding Layers:\n",
    "     \n",
    "•\tIncrementally add layers using both APIs:\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "     \n",
    "6. Configuring Layer Parameters:\n",
    "•\tSet activation functions and units:\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "7. Data Preprocessing:\n",
    "•\tNormalization & One-Hot Encoding:\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "data = pd.read_csv('data.csv').fillna(method='ffill').drop_duplicates()\n",
    "normalized_data = MinMaxScaler().fit_transform(data)\n",
    "encoded_data = OneHotEncoder(sparse=False).fit_transform(data[['category_column']])\n",
    "            \n",
    "8. Splitting Data:\n",
    "•\tSplit data into training, validation, and test sets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    " \n",
    "9. Recurrent Neural Networks (RNN):\n",
    "•\tRNNs process sequences where outputs depend on previous inputs.\n",
    "•\tExample:\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.SimpleRNN(64, activation='tanh', input_shape=(10, 1)))            \n",
    "model.add(layers.Dense(1))\n",
    "            \n",
    "10. Cleaning Text Data:\n",
    "•\tNormalize and clean text for NLP:\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "text = \"This is an example sentence!\"\n",
    "text = re.sub(r'[^\\w\\s]', '', text.lower()).strip()\n",
    "    \n",
    "11. Tokenization:\n",
    "•\tProcess of dividing text into tokens:\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Hello world!\"\n",
    "word_tokens = word_tokenize(text)\n",
    "    \n",
    "12. Text to Sequence:\n",
    "•\tConvert text to numerical sequences:\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "texts = [\"I love machine learning.\"]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "         \n",
    "13. Training Neural Networks with Backpropagation\n",
    "Backpropagation is the process of fine-tuning the weights of a neural network based on the error rate obtained in the previous epoch.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "________________________________________\n",
    "14. Evaluating Model Performance\n",
    "Model performance can be evaluated using metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict_classes(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "________________________________________\n",
    "15. Hyperparameter Tuning\n",
    "You can improve the model by tuning hyperparameters like learning rate, batch size, and number of epochs.\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_val, y_val))\n",
    "________________________________________\n",
    "16. Early Stopping in Neural Networks\n",
    "Early stopping is used to stop training when the model's performance on the validation set stops improving.\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "________________________________________\n",
    "17. Batch Normalization\n",
    "Batch normalization is used to speed up training and make the model more stable.\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "________________________________________\n",
    "18. Dropout Regularization\n",
    "Dropout is a regularization technique that prevents overfitting by randomly dropping neurons during training.\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "________________________________________\n",
    "19. Transfer Learning\n",
    "Transfer learning involves using a pre-trained model and adapting it to your task.\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "model = Sequential([base_model, Dense(10, activation='softmax')])\n",
    "________________________________________\n",
    "20. Custom Loss Function\n",
    "You can define your custom loss function to handle specific problems.\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
